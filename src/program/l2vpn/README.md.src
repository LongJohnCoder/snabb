# Layer-2 VPN  (`program.l2vpn.l2vpn`)

## <a name="overview">Overview</a>

### General Architecture

The program `program.l2vpn.l2vpn` (abbreviated by `l2vpn` in this
document) implements a virtual multi-port Ethernet switch on top of a
plain IP (v4/v6) packet-switched network based on the architecture
described in [RFC 4664](https://tools.ietf.org/html/rfc4664).  From a
customer perspective, the service consists of a set of physical ports
attached to the provider's network at arbitrary locations with the
semantics of a simple Ethernet switch, a.k.a. _learning bridge_.  Each
such port is called an _attachment circuit_ (AC).

<a name="overview_MAC_learning"></a>Conceptually, the processing of an
Ethernet frame entering the virtual switch through any of the ACs
proceeds as follows.  First, the switch extracts the MAC source
address and adds it to the list of addresses reachable through the
ingress AC port.  This list is called the _MAC destination address
table_, or _MAC table_ for short.  Next, the switch extracts the MAC
destination address and forwards the frame according to the following
rules:

   * If the destination address is not found in any of the MAC tables
     or is a multicast address, the frame is forwarded to all ACs
     except the one on which it was received

   * If the destination address is found in in one of the MAC tables,
     it is forwarded out of the corresponding port only

Because the virtual switch is composed of distinct physical devices,
only the MAC tables of local ACs are directly accessible.  Therefore,
a mechanism is needed that provides discovery of the remote device to
which the destination is connected and a method to transfer the
Ethernet frame to that device.

The service is implemented by connecting the set of _provider edge_
devices (PE) that host the attachment circuits by tunnels which
transport the Ethernet frames within IP packets (IPv4 or IPv6) using
an appropriate encapsulation mechanism called the _tunneling
protocol_.  Each such tunnel is called a _pseudowire_ (PW) and
contains exactly two endpoints.  Conceptually, each PE implements its
own learning bridge to which all local attachment circuits and
pseudowires are connected.  The bridge maintains MAC tables for all of
those ports.  In general, the resulting topology contains loops and a
mechanism is needed that prevents packets from entering any such loop,
such as the _spanning tree protocol_ (SPT).  This implementation uses
a full mesh of PWs together with _split horizon_ forwarding to avoid
loops without the need for SPT.  In this model, each PE maintains a PW
to every other PE (this is the full mesh) with the rule that a frame
received on any PW is never forwarded to any other PW (this is the
split-horizon rule).  The trade-off with respect to SPT is that the
number of PWs in the system scales quadratically with the number of PE
devices.

The following figure illustrates the model with a setup that contains
four PEs, six ACs and six PWs.

    DIAGRAM: L2VPN_ARCH
             
             +=--Virtual switch------------------------------------------------------+
             |                                                                       |
             |                                                                       |
             |   +----------+                                         +----------+   |
             |   |          |                                         |          |   |
    AC  <----|-->+          +<--------------------------------------->+          |   |
             |   | PE Site A|                                         | PE Site B+<--|----> AC
    AC  <----|-->+          |             +-------------------------->+          |   |
             |   |          |             |                           |          |   |
             |   +---+--+---+             |                           +------+---+   |
             |       ^  ^                 |                                  ^       |
             |       |  |                 |    Full mesh of pseudowires      |       |
             |       |  |                 |                                  |       |
             |       |  |                 |                                  |       |
             |       |  |                 |                                  |       |
             |       |  |                 |                                  |       |
             |       |  +-------------------------------------------------+  |       |
             |       |                    |                               |  |       |
             |       |                    |                               |  |       |
             |       |                    |                               |  |       |
             |       |                    |                               |  |       |
             |       |                    |                               |  |       |
             |       V                    |                               V  V       |
             |   +---+------+             |                           +---+--+---+   |
             |   |          |             |                           |          |   |
    AC  <----|-->+          +<------------+                           |          |   |
             |   | PE Site C|                                         | PE Site D+<--|----> AC
    AC  <----|-->+          +<--------------------------------------->+          |   |
             |   |          |                                         |          |   |
             |   +----------+                                         +----------+   |
             |                                                                       |
             |                                                                       |
             +=----------------------------------------------------------------------+
    
With the nomenclature from RFC 4664, a setup with exactly two ACs is
called a _virtual private wire service_ (VPWS).  It is essentially a
virtual Ethernet cable.  This is also referred to as a point-to-point
L2 VPN.

A setup with more than two ACs (and, in general, more than two PEs) is
called _virtual private LAN service_ (VPLS).  This is also referred to
as a multi-point L2 VPN.  A VPWS is a degenerate case of a VPLS.

#### <a name="ac-modes">Attachment Circuit Operational Modes</a>

ACs can be operated in one of two modes called _port-mode_ and
_VLAN-mode_.  If the AC operates in port-mode, all Ethernet frames
arriving on the port are mapped to a single VPLS instance and
forwarded unchanged.  In particular, any VLAN tags (e.g. according to
IEEE 802.1Q or 802.1ad) are ignored and only the source and
destination MAC addresses are examined for the purpose of forwarding.

If the AC operates in VLAN-mode it is effectively configured as a VLAN
trunk from a customer perspective. Ethernet frames arriving on the
port are mapped to different VPLS instances based on the outermost
VLAN tag, which is referred to as the _service-delimiting tag_.  The
tag is stripped from the packet by the ingress PE before it is
forwarded into the VPLS to which the tag is mapped.  When the PE
receives a frame from a PW, it adds the tag of the respective VPLS to
the packet before forwarding it on the AC port.  Untagged frames
(i.e. those that do not carry any kind of VLAN tag) can be mapped to a
particular VPLS, if desired, to support the notion of a "native" VLAN
on the AC trunk.  In this case, Ethernet frames belonging to this VPLS
are forwarded unchanged between the AC and the VPLS.

The service-delimiting tags are strictly local in nature: they only
provide the mapping from VLANs transported on an AC to a particular
VPLS instance on the PE device itself.  In particular, there is no
correlation at all between such mappings on distinct PE devices, no
matter which VPLS instances they serve, including the case where some
of the ACs of a VPLS work in port-mode while others work in VLAN-mode.

There exist different schemes for applying VLAN tags to an Ethernet
frame which differ in the value used in the _type_ field of the
Ethernet header (a.k.a. the _Ethertype_).  The following encapsulation
modes are supported.

   * `dot1q`: Ethertype `0x8100` (https://en.wikipedia.org/wiki/IEEE_802.1Q)
   * `dot1ad`: Ethertype `0x88a8` (https://en.wikipedia.org/wiki/IEEE_802.1ad)
   * `raw`: configurable Ethertype

The `raw` mode can be used to configure a non-standard Ethertype,
e.g. `0x9100` which is still in use by some vendors.  To be more
precise, all encapsulation schemes must adhere to the generic layout of
the 32-bit tag as defined by 802.1Q (the numbers refer to the number
of bits used by the respective fields)

    DIAGRAM: DOT1Q_TAG
    
    +---------------------+-----+-----+----------+
    |         16          |  3  |  1  |    12    |
    +---------------------+-----+-----+----------+
    |                     |         TCI          |
    |        TPID         +-----+-----+----------+
    |                     | PCP | DEI |    VID   |
    +---------------------+-----+-----+----------+

The values above actually refer to the _tag protocol identifier_
(TPID), which overlaps with the Ethertype in case of the outermost
tag.  The 12-bit _VLAN identifier_ (VID) is part of the _tag control
information_ (TCI) together with the _priority code point_ (PCP) and
the _drop eligible indicator_ (DEI).

Any subsequent tags that might exist in the packet are ignored and
passed on unchanged.

Both types of ACs are completely transparent for any kind of
L2-protocol that might be transported over it (e.g. SPT, VTP, CDP,
etc.).  As a consequence, no special configuration is required to
enforce this behavior like on many conventional networking devices.
In particular, the technique of _protocol tunneling_ is never
required.

### VPN Termination Point

In the present implementation, the role of the PE is played by one or
more instances of the `program.l2vpn.l2vpn` module.  Each instance is
called a _VPN termination point_ (VPNTP), which may contain any number
of VPLS instances, each of which is comprised of the following
elements

   * A set of pseudowire endpoints, one for each remote PE which is
     part of the VPLS

   * The set of attachment circuits which belong to the VPLS (there
     can be more than one local AC per VPLS, but the customer is
     responsible for loop-prevention if these ACs are connected to a
     another switch)

   * A learning bridge which connects all PWs and ACs of the VPLS
     instance.  The bridge implements MAC learning and split-horizon
     forwarding for the group of PWs

   * An upstream interface, which is connected to the ISPs network and
     transports the traffic of all PWs

Ethernet frames entering the VPNTP through an AC are switched to
another AC or one of the PWs according to the bridge's forwarding
table.  A PW encapsulates the Ethernet frame in an IP packet and
transmits it to the remote PE device.

IP frames arriving on a PW from a remote PE are decapsulated and the
resulting Ethernet frame is switched to an AC by the bridge.

A PW is uniquely identified by the tuple (address family, source
address, destination address, VC ID). The address family is either
IPv4 or IPv6 and the addresses must be valid within that family.  The
VC ID is used to distinguish PWs between the same pair of addresses.

Apart from the IP header used to transport the Ethernet frames across
the network, a PW also uses a particular "upper layer" protocol header
called the _tunnel protocol_ to identify the payload and carry
additional information relevant for the tunnel, for example the VC ID.

The tunnel protocol is a property of a PW (and, consequently, each end
of a PW must be configured to use the same protocol).  However, it is
possible to mix PWs with different tunnel protocols and/or address
families in the same VPLS.  The supported tunnel protocols are listed
in the next section.

The entire architecture is shown in the following figure.  For
simplicity, the distinction between IPv4 and IPv6 is not shown here.
In the implementation, there exist separate data paths with identical
functionality for both address families.

In the case depicted below, all VPLS instances are configured to share
the same uplink, but there is no restriction in this respect,
i.e. each VPLS or groups of VPLS could use different uplinks.  The box
labelled `ND` is a module that provides neighbor discovery to
determine the MAC address of the adjacent router in the outbound
direction and let the adjacent router discover the local MAC address
when sending packets in the inbound direction.  The ND module is the
feature that identifies the uplink as a "L3-port", while the ACs are
L2-ports without the need for neighbor discovery.

The box labelled `Frag` is a collection of apps that performs IP
fragmentation and reassembly for packets destined to any of the
pseudowires connected to the dispatcher. Initially, the path MTU
(PMTU) for all outgoing packets is equal to the MTU of the uplink
port, but the fragmenter implements standard PMTU discovery (PMTUD):
if any of the packets experiences an MTU bottleneck (i.e. a link along
the path to the destination whose MTU is smaller than that of the
uplink port), the corresponding device will generate an appropriate
ICMP _Packet-Too-Big_ (PTB) message, which will be processed by the
fragmenter to populate a per-destination PMTU cache.  From that point
on, packets sent to that destination will be fragmented for that PMTU
instead of the uplink's MTU.  The cached PMTUD is discarded after 10
minutes to allow the system to discover an increase of the PMTU,
e.g. after a routing change.

The box labelled _dispatcher_ contains a collection of apps which
de-multiplex the traffic coming in from the uplink based on the pair
of source and destination address, tunnel protocol and VC ID in order
to pass the Ethernet frame carried in the payload to the proper VPLS
instance.  In the reverse direction, the dispatcher adds the
appropriate tunnel and transport headers and passes the packet on for
transmission on the uplink.

    DIAGRAM: VPN-TP
    
    ------------------------------------------ Decapsulation --------------------------->
    <----------------------------------------- Encapsulation ----------------------------
                                                            +=------------------+
                                                            |  VPLS             |
                                                            |                   |
                                                        PW  |     +---------+   |
                                                      +---------->+         |   |
                                                      |     :     |         +<-------> AC
                                                      |     |     | Bridge  |   :  .
                                                      |     |     |         |   |  .
                                                      | PW  |     |         +<-------> AC
                                                      |+--------->+         |   |
                                                      ||    |     +---------+   |
                                    +------------+    ||    +=------------------+
                                    |            +<---+|              .
                +----+   +------+   |            +<----+              .
    Uplink <--->+ ND +<->+ Frag +<->+ dispatcher |                    .
                +----+   +------+   |            +<----+              .
                                    |            +<---+|              .
                                    +------------+    ||    +=------------------+
                                                      ||    |     +---------+   |
                                                      |+--------->+         |   |
                                                      | PW  :     |         +<-------> AC
                                                      |     |     | Bridge  |   :  .
                                                      |     |     |         |   |  .
                                                      |     |     |         +<-------> AC
                                                      +---------->+         |   |
                                                        PW  |     +---------+   |
                                                            |                   |
                                                            |  VPLS             |
                                                            +=------------------+

Any of the ACs or uplinks can be implemented either as physical
interfaces or as VLAN-mode sub-interfaces of a physical interface
which is configured as a VLAN trunk.  In this case, an additional
module called a _VLAN multiplexer_ is pushed between the physical
interface and the next module.  The following diagram shows the
topology when two ACs share one physical interface configured as a
VLAN trunk, where the ACs uses the VLAN IDs 100 and 200, respectively.


    DIAGRAM: AC-VLAN-TRUNK
    
          +---------+
          |         |  AC
          | Bridge  +<------+
          |         |       |VID 100
          +---------+       V
                         +--+--+   
                         | VLAN|  trunk
                         | Mux +<-------> interface
                         |     |
                         +--+--+
          +---------+       ^
          |         |       |VID 200
          | Bridge  +<------+
          |         |  AC
          +---------+    

If two uplinks share one physical interface, the topology looks as
follows.  The ND modules now attach to the VLAN ports of the
multiplexer and its trunk port connects to the interface.

    DIAGRAM: UPLINK-VLAN-TRUNK
    
                                +----+    +------+    +------------+
                          +---->+ ND +<-->+ Frag +<-->+ dispatcher +<--
                   VID 100|     +----+    +------+    +------------+
                          V
                       +--+--+
                trunk  | VLAN|
    interface <------->+ Mux |
                       |     |
                       +--+--+
                          ^
                   VID 200|     +----+    +------+    +------------+
                          +---->+ ND +<-->+ Frag +<-->+ dispatcher +<--
                                +----+    +------+    +------------+

It is possible to let all ACs and uplinks share the same physical
port, providing a "VPNTP-on-a-stick" configuration.

### <a name="tunnel_protos">Tunneling protocols</a>

The following tunneling protocols are supported.

#### GRE

The GRE protocol version 0 is supported according to RFCs
[1701](https://tools.ietf.org/html/rfc1701),
[2784](https://tools.ietf.org/html/rfc2784) and
[2890](https://tools.ietf.org/html/rfc2890) with the restriction that
only the `key` extension is available, which is set to the _VC ID_
assigned to the PW.  The implementation limits the range of valid VC
IDs from 1 to 32767, see below. The 2-byte protocol identifier carried
in every GRE header is set to the value `0x6558` assigned to
"Transparent Ethernet Bridging" (see the [IEEE Ethertype
assignments](http://standards-oui.ieee.org/ethertype/eth.txt))

#### <a name="tunnel_keyed_ipv6">Keyed IPv6 Tunnel</a>

The keyed IPv6 tunnel is a stripped down version of the L2TPv3
protocol as specified by
[`RFC8159`](https://tools.ietf.org/html/rfc8159).

The protocol is only defined for transport over IPv6 and does not make
use of the regular L2TPv3 control channel.  The header contains only
two fields:

   * A 32-bit _session ID_

   * A 64-bit _cookie_

Since the pseudowire associated with a tunnel is already uniquely
identified by the IPv6 addresses of the endpoints, the session ID is
not actually needed to assign a packet to a particular VPLS instance.
The RFC recommends setting the session ID to the value `0xFFFFFFFF`,
which is enforced by this implementation.

The cookie is used to protect against spoofing and brute-force blind
insertion attacks.  Each endpoint is configured with a _remote
cookie_, which is put into outgoing packets and a _local cookie_ which
is compared to the cookie received in incoming packets.  A cookie
should be chosen at random and the local and remote cookies should be
chosen independently.

Note that the specification implies that the payload must consist of
an Ethernet frame, since the tunnel header does not contain any
information about the payload (like the protocol field of GRE).

### Pseudowire Signalling and Monitoring

#### <a name="control-channel">Pseudowire Control-Channel</a>

Each pseudowire implements an optional in-band _control channel_ (CC),
which provides two types of services:

   1. Parameter advertisement. One end of the PW can advertise local
   parameters to its peer, either for informational purposes or to
   detect misconfigurations.

   2. Connection verification. Unidirectional connectivity is
   verified by sending periodic heartbeat messages.  A monitoring
   station that polls both endpoints can determine the full
   connectivity status of the PW (see below for details).

This proprietary protocol performs a subset of functions that are
provided by distinct protocols in standardized PW implementations
based on MPLS or L2TPv3.  Both of these provide a
"maintenance/signalling protocol" used to signal and set up a PW.
For MPLS, this role is played by LDP (in "targeted" mode).  For
L2TPv3, it is provided by a control-channel which is part of L2TPv3
itself (unless static configuration is used).

The connection verification is provided by another separate protocol
called VCCV ([RFC5085](https://tools.ietf.org/html/rfc5085)).  VCCV
requires a signalling protocol to be used in order to negotiate the
"CC" (control channel) and "CV" (connection verification) modes to be
used by the peers.  Currently, VCCV is only specified for MPLS/LDP and
L2TPv3, i.e. it is not possible to implement it for any of the
tunneling protocols currently supported by `l2vpn` (it is not
specified for GRE and the keyed IPv6 tunnel does not make use of the
L2TPv3 control channel).

The simple control-channel protocol provided here uses a TLV encoding
to transmit information from one peer to the other.  The data is
carried in-band, i.e. in packets that use the same encapsulation as
the data traffic, which has the advantage of _fate sharing_: if the
data-plane breaks, the control-channel is disrupted as well and the
failure is detected automatically.

A protocol-dependent mechanism must be used to differentiate control
traffic from data traffic.  For GRE, the VC ID used for the control
channel is derived from the VC ID of the PW by adding the value
0x8000.  This convention also restricts the range of VC IDs available
for PWs to 0x0000 - 0x7FFF.

For L2TPv3, the VC ID is not configurable (the PW identitiy is
determined by the sourde and destination IPv6 addrresses alone).  It
uses the session ID `0xFFFFFFFF` for the PW and `0xFFFFFFFE` for the
control channel.

The following items are supported by the control-channel.

   * `heartbeat`: an unsigned 16-bit number that specifies the
     interval in seconds at which the sender emits control frames.  It
     is used by the receiver to determine whether it is reachable by
     the peer.

   * `mtu`: The MTU of the attachment circuit (or virtual bridge port
     in case of a multi-point VPN) of the sender.  The receiver
     verifies that it matches its own MTU and marks the PW as
     non-functional if this is not the case.  The MTU is carried as an
     unsigned 16-bit number in the TLV.

   * `if_description`: Interface description (TODO)

The dead-peer detection proceeds as follows.  The local endpoint
records the heartbeat interval advertised by the remote end over the
control channel.  The peer is declared dead if no control message is
received for a locally configured multiple of that interval.  This
multiplication factor is called the _dead factor_.

For example, if the remote peer advertises a heartbeat of 20 seconds
and the local peer uses a value of 3 for its dead factor, the
pseudowire is declared to be down 3x20 = 60 seconds after the last
control message has been received.  This notion of unreachability is
obviously unidirectional (i.e. packets from the remote end do not
reach the local end of the pseudowire).  To determine the
bi-directional status of the PW, the reachability status of both
endpoints must be obtained by the operator.

## Configuration

The configuration for the `l2vpn` program is represented as an
instance of a YANG schema expressed in the syntax of YANG itself
[(`RFC6020`)](https://tools.ietf.org/html/rfc6020).  It consists of
the following major sections in the form of _containers_ or _lists_
(in the context of YANG).

```
l2vpn-config {
  luajit {}
  snmp {}
  interface {}
  peers {}
  transport {}
  vpls {}
}
```

Refer to the [complete YANG schema](#yang-schema) for the definitions
of each section.  The following sections show fragments of the YANG
schema with an obvious pseudo-notation to denote altrenative values
for options and optional items. E.g.

```
foo {
  bar a | b | c;
  [ baz <baz> ; ]
}
```

would describe a container called `foo`, which contains a leaf node
called `bar` that can take one of three values `a`, `b` or `c` and an
optional leaf node called `baz` whose value will be referred to in the
text as `<baz>`.

### Section `luajit`

This section allows the configuration of features of the LuaJIT
runtime system.  An _option_ can be any item from the *Parameter*
table of the [LuaJIT CLI reference](http://luajit.org/running.html).
Recommended option settings for the `l2vpn` program are as follows

```
luajit {
  option "sizemcode=2048";
  option "maxmcode=8192";
  option "maxtrace=2000";
  option "maxsnap=6000";
  option "maxrecord=8000";
}
```

Valid values for `dump/option` can be found in the [LuaJIT Git
repository](https://github.com/LuaJIT/LuaJIT/blob/master/src/jit/dump.lua).
The value `+rs` gives the most details.  It should only be used for
debugging purposes.  A value of `t` is fairly light-weight and only
shows the start, end and abort of JIT traces.  Example configuration:

```
luajit {
  dump {
    enable true;
    option "+rs";
    file "/tmp/dump-%p";
}
```

Note that the pattern `%p` will be replaced by the ID of the worker
process in which the instance of LuaJIT is executing.  This allows
keeping separate dump files for the data-plane and control-plane
processes.

### Section `snmp`

This section configures global properties related to [SNMP](#SNMP).
This only affects SNMP for MIBs related to interfaces.  SNMP for
pseudowires is always enabled.

### Section `interface`

Each interface known to `l2vpn` must be configured as an item of a
list of all interfaces.  Each interface is uniquely identified by the
value of the `name` leaf node. See the section about [interface
abstraction](#interface-abstraction) for more details.

### Section `peers`

This section defines an inventory of all systems that play the role of
*PE* devices for any VPLS instances defined by the configuration.
They are referred to as _peers_ rather than PE devices in this
context.  Each peer has a unique name and a list of IPv4 and IPv6
addresses associated with it.  Each address can be used as one
endpoint of a pseudowire as detailed in the
[`transport`](#transport-config) section.

The list of peers is divided into a local part and a remote part.
There must be exactly one local peer and at least one remote peer.
The local peer defines the name and addresses of the local system.

```
peers {
  local {
    name <peer-name>;
    [ endpoint {
        name <endpoint-name>;
        ipv4 <ipv4-address> | ipv6 <ip6-address>;
      } ]
    [ ... ]
  }
  remote {
    name <peer-name>;
    [ endpoint {
        name <endpoint-name>;
        ipv4 <ipv4-address> | ipv6 <ip6-address>;
      } ]
    [ ... ]
  }
  [ remote ... ]
}
```

The following fragment defines a set of three peers.

```
peers {
  local {
    name "local";
    endpoint {
      name "l_v4";
      ipv4 "192.0.2.1";
    }
    endpoint {
      name "l_v6";
      ipv6 "2001:db8::1";
    }
  }
  remote {
    name "remote1";
    endpoint {
      name "r1_v4";
      ipv4 "192.0.2.2";
    }
    endpoint {
      name "r1_v6";
      ipv6 "2001:db8::2";
    }
  }
  remote {
    name "remote2";
    endpoint {
      name "r2_v4";
      ipv4 "192.0.2.3";
    }
  }
}
```

### Section <a name="transport-config">`transport`</a>

A `transport` is an ordered pair of addresses of a specific address
family that make up the endpoints of a pseudowire.  The local endpoint
must be chosen from the local peer and the remote endpoint must be
chosen from any of the remote peers.

The reference to an address is composed of the name of the peer and
the name of the endpoint within that peer.  It is an error to
reference an address whose address family doesn't match that of the
transport.

Each transport has a unique name by which it can be referred to in the
context of a pseudowire assigned to a VPLS instance.

```
transport {
  name <transport-name>;
  address-family ipv4 | ipv6;
  local {
    peer <peer-name>;
    endpoint <endpoint-name>;
  }
  remote {
    peer <peer-name>;
    endpoint <endpoint-name>;
  }
  ipsec {
  }
}
```

The `ipsec` container is descriped in the [IPsec](#ipsec) section.

For example, given the peers from the previous section, the following
fragment defines three transports between the peers.

```
transport {
  name "r1_v4";
  address-family ipv4;
  local {
    peer "local";
    endpoint "l_v4";
  }
  remote {
    peer "remote1";
    endpoint "r1_v4";
  }
}
transport {
  name "r1_v6";
  address-family ipv6;
  local {
    peer "local";
    endpoint "l_v6";
  }
  remote {
    peer "remote1";
    endpoint "r1_v6";
  }
}
transport {
  name "r2_v4";
  address-family ipv4;
  local {
    peer "local";
    endpoint "l_v4";
  }
  remote {
    peer "remote1";
    endpoint "r2_v4";
  }
}
```

### Section <a name="vpls-instance-config">`vpls`</a>

This section configures an instance of a VPLS as an item of a list of
all VPLS instances.  Each VPLS is unqiuely identified by the value of
the `name` leaf node.  The structure of a VPLS configuration is as
follows

```
vpls {
  name <name>;
  [ description <description>; ]
  uplink <uplink>;
  mtu <mtu>;
  bridge {}
  attachment-circuit {}
  [ attachment-circuit {}
    ... ]
  pseudowire {}
  [ pseudowire {}
    ... ]
}
```

A brief documentation of these items is included in the [YANG
schema](#yang-schema). Since this is the central piece of
configuration of the `l2vpn` program, a more detailed description of
the items follows here.

   * `name`

      The name doesn't have any particular significance but must be
      unique among all `vpls` instances.  It is also used in
      diagnostic output.

   * `description`

     A human readable description of the purpose of the VPLS instance.

   * `mtu`

     The MTU in bytes of all ACs connected to the VPLS instance
     including the Ethernet header (but neither the CRC, SOF nor
     preamble, also see [the definition of the interface MTU](#MTU)).
     As with any Ethernet bridge (and IP subnet), it is mandatory that
     all interfaces connected to it use the same MTU, i.e. it is a
     property of the VPLS.  To avoid misconfigurations, it is also
     communicated to the remote endpoints of all pseudowires through
     the control channel to make the MTU coherent across the entire
     virtual switch.

     Since the MTU of the ACs is configured independently in the
     `interfaces` section, an error is raised when the MTU setting
     between the VPLS and any of its ACs differs.  If an AC is in
     [VLAN-mode](#ac-modes), the size of the service-delimiting tag (4
     bytes) is subtracted from the MTU of the sub-interface before it
     is compared to the VPLS MTU.

   * `uplink`

     A reference to an interface defined in the global `interfaces`
     definition, for example
     ```
     uplink "TenGigE0/1";
     ```

     or

     ```
     uplink "TenGigE0/1.100";
     ```

     The referenced interface must be configured as a L3-port, i.e. it
     must contain an `address-families` section.  (see the [interface
     abstraction](#interface-abstraction) section for details)

   * `bridge`

     If the VPLS contains more than one pseudowire or connects more
     than one AC, a bridge is needed to perform forwarding of Ethernet
     frames.  The configuration of the bridge is supplied in the
     `bridge` container, which allows the choice between two types of
     bridges named `learning` and `flooding`

     ```
     bridge {
       learning {
         mac-table {
           size 1024;
           timeout 600;
           verbose false;
           max-occupy 0.4;
         }
       }
     }
     ```

     ```
     bridge {
       flooding {}
     }
     ```

     If `flooding` is selected, the bridge will send copies of all
     Ethernet frames received on any port to all other ports,
     i.e. every frame will be flooded across the entire VPLS.

     If `learning` is selected, the bridge performs MAC learning as
     described in the [overview](#overview_MAC_learning).  The
     `mac-table` container sets the properties of the MAC table
     associated with the bridge.

        * `size`

           The number of MAC addresses that the table can store

        * `timeout`

          The number of seconds after which an inactive MAC address is
          purged from the table.

        * `verbose`

          If set to `true`, the current usage of the table is printed
          every `timeout` seconds, including the number of used
          entries, the total number of entries and their ratio. The
          latter is a measure of the occupancy of the hash table.

        * `max-occupy`

          The maximum occupancy (ratio of used entries and total
          number of entries) at which the MAC table size is increased
          by a factor of 2.

     If the VPLS is a VPWS (i.e. a point-to-point VPN with a single AC
     at each end), an actual bridge is not necessary.  In this case,
     the bridge module is not instantiated and the local end of the
     pseudowire is "short-circuited" to the AC.

   * `attachment-circuit`

     Each attachment circuit is configured as a separate instance of
     the `attachment-circuit` list.  Each instance contains a name,
     which must be unique but has otherwise no significance, and the
     name of the interface, for example

     ```
     attachment-circuit {
       name "AC_1";
       interface "TenGigE0/0";
     }
     attachment-circuit {
       name "AC_2";
       interface "TenGigE0/1.100";
     }
     ```

     defines two ACs named `AC_1` and `AC_2`.  The first one refers to
     a physical interface called `TenGigE0/0` while the other refers
     to a VLAN-based sub-interface associated with the physical
     interface called `TenGigE0/1`.  Interfaces configured as AC must
     be L2-ports, i.e. they must not have an `address-families`
     configuration associated with them (see the [interface
     abstraction](#interface-abstraction) section for details).

   * `pseudowire`

     Each pseudowire that is to be associated with the VPLS must be
     configured with an instance of the `pseudowire` list containing
     the following elements

      * `name`

         An arbitrary name which must be unique among the pseudowires
         of a VPLS

      * `vc-id`

        A number in a range specific to the encapsulation type that
        serves as a distinguisher between pseudowires with the same
        transport addresses. The tuple (`local-address`,
        `remote-address`, `vc-id`) must uniquely identify each
        pseudowire within an instance of the `l2vpn` program.

      * `transport`

        The name of the transport to use for this pseudowire.  It must
        match exactly the value of the _name_ node of the transport.

      * `tunnel`

        The type and configuration of the encapsulation used on top of
        the transport.  Currently, two types are supported

        ```
        tunnel {
          l2tpv3 {
            local-cookie <value>;
            remote-cookie <value>;
          }
        }
        ```

        ```
        tunnel {
          gre {}
        }
        ```

        The `<value>` for the L2TPv3 cookies must be Lua strings that
        evaluate to 8-byte binary objects.  For example, the sequence
        of 8 bytes of zero can be represented as
        "\x00\x00\x00\x00\x00\x00\x00\x00".  The value of the
        `local-cookie` is what is expected to be found in incoming
        packets while the vlaue of the `remote-cookie` is placed into
        outgoing packets.

        The GRE encapsulation requires no additional configuration.

      * `control-channel`

        A container that specifies the control channel configuration
        for the pseudowire

        ```
        control-channel {
          enable true;
          heartbeat 5;
          dead-factor 3;
        }
        ```

        The `enable` leaf node specifies whether the control channel
        should be enabled. `heartbeat` is the interval in seconds at
        which the pseudowire sends control messages to its peer.  This
        number itself is transmitted within the control message as the
        `heartbeat` parameter. The value of the `dead_factor`, which
        must be an integer, is used to detect when the remote endpoint
        can no longer reach the local endpoint, see the [description
        of the control-channel](#control_channel) for details.

### Examples

#### Point-to-Point VPN

This VPN consists of two endpoints called `A` and `B` with local
addresses `2001:db8:0:1:0:0:0:1` and `2001:db8:0:1:0:0:0:2`,
respectively.  The IPv6 subnet and address on the uplink of endpoint
`A` is `2001:db8:0:C101:0:0:0:2/64` and the default route points to
`2001:db8:0:C101:0:0:0:1`.  For endpoint `B`, the corresponding addresses are
`2001:db8:0:C102:0:0:0:2/64` and `2001:db8:0:C102:0:0:0:2`.

The encapsulation on the (single) pseudowire is L2TPv3 with different
cookies in both directions.

The MTU on the ACs allows for untagged 1500-byte IP packets while the
uplinks have a much larger MTU. 

There is no explicit bridge configuration and the default
flooding-bridge is optimized away.

Endpoint `A`:

```
l2vpn-config {
  snmp {
    enable true;
  }
  interface {
    name "TenGigE0/1";
    description "uplink";
    driver {
      path "apps.intel_mp.intel_mp";
      name "Intel";
      config "{ pciaddr = '0000:04:00.1', rxq = 0, txq = 0 }";
    }
    mtu 9206;
    address-families {
      ipv6 {
        address "2001:db8:0:C101:0:0:0:2";
        next-hop "2001:db8:0:C101:0:0:0:1";
      }
    }
  }
  interface {
    name "TenGigE0/0";
    description "AC";
    driver {
      path "apps.intel_mp.intel_mp";
      name "Intel";
      config "{ pciaddr = '0000:04:00.0', rxq = 0, txq = 0 }";
    }
    mtu 9206;
  }
  peers {
    local {
      name "A";
      endpoint {
        name "local";
        ipv6 "2001:db8:0:1:0:0:0:1";
      }
    }
    remote {
      name "B";
      endpoint {
        name "remote";
        ipv6 "2001:db8:0:1:0:0:0:2";
      }
    }
  }
  transport {
    name "transport";
    address-family "ipv6";
    local {
      peer "A";
      endpoint "local";
    }
    remote {
      peer "B";
      endpoint "remote";
    }
  }
  vpls {
    name "myvpn";
    description "Endpoint A of a point-to-point L2 VPN";
    uplink "TenGigE0/1";
    mtu 1514;
    attachment-circuit {
      name "ac_A";
      interface "TenGigE0/0";
    }
    pseusowire {
      name "pw_B";
      vc-id 0;
      transport "transport";
      tunnel {
        l2tpv3 {
          local-cookie "\x00\x11\x22\x33\x44\x55\x66\x77";
          remote-cookie "\x77\x66\x55\x44\x33\x33\x11\x00";
        }
      }
      control-channel {
        heartbeat 2;
        dead-factor 4;
      },
    }
  }
}
```

Endpoint `B`:

```
l2vpn-config {
  snmp {
    enable true;
  }
  interface {
    name "TenGigE0/1";
    description "uplink";
    driver {
      path "apps.intel_mp.intel_mp";
      name "Intel";
      config "{ pciaddr = '0000:04:00.1', rxq = 0, txq = 0 }";
    }
    mtu 9206;
    address-families {
      ipv6 {
        address "2001:db8:0:C102:0:0:0:2";
        next-hop "2001:db8:0:C102:0:0:0:1";
      }
    }
  }
  interface {
    name "TenGigE0/0";
    description "AC";
    driver {
      path "apps.intel_mp.intel_mp";
      name "Intel";
      config "{ pciaddr = '0000:04:00.0', rxq = 0, txq = 0 }";
    }
    mtu 9206;
  }
  peers {
    local {
      name "B";
      endpoint {
        name "local";
        ipv6 "2001:db8:0:1:0:0:0:2";
      }
    }
    remote {
      name "A";
      endpoint {
        name "remote";
        ipv6 "2001:db8:0:1:0:0:0:1";
      }
    }
  }
  transport {
    name "transport";
    address-family "ipv6";
    local {
      peer "B";
      endpoint "local";
    }
    remote {
      peer "A";
      endpoint "remote";
    }
  }
  vpls {
    name "myvpn";
    description "Endpoint B of a point-to-point L2 VPN";
    uplink "TenGigE0/1";
    mtu 1514;
    attachment-circuit {
      name "ac_B";
      interface "TenGigE0/0";
    }
    pseudowire {
      name "pw_A";
      vc_id = 0;
      transport "transport";
      tunnel {
        l2tpv3 {
          local-cookie "\x77\x66\x55\x44\x33\x33\x11\x00";
          remote-cookie "\x00\x11\x22\x33\x44\x55\x66\x77";
        }
      }
      control-channel {
        heartbeat 2;
        dead_factor 4;
      }
    }
  }
}
```

### <a name="yang-schema">Complete YANG schema</a>

```
module snabb-l2vpn-v1 {
  namespace snabb:l2vpn;
  prefix l2vpn;

  import ietf-inet-types { prefix inet; }
  import ietf-yang-types { prefix yang; }

  organization "SWITCH";
  contact "Alexander Gall <alexander.gall@switch.ch>";
  description
    "Configuration for the Snabb Switch L2VPN Program";

  revision 2019-02-03 {
    description
      "Initial revision.";
  }

  container l2vpn-config {
    description
      "Configuration for the Snabb L2VPN program.";

    container luajit {
      description
        "LuaJIT runtime settings";

      leaf-list option {
        type string;
        description
          "A LuaJIT runtime option.";
      }
      container dump {
        leaf enable {
          type boolean;
          default false;
          description
            "Whether to enable the JIT dump facility.";
        }
        leaf option {
          type string;
          description
            "Options for the LuaJIT trace dumper.";
        }
        leaf file {
          type string;
          default "/tmp/dump";
          description
            "Location of the LuaJIT dump file.  The file name may contain the
             string '%p', which will be replaced by the ID of the
             worker process in which the LuaJIT instance is executing";
        }
      }
    }

    container snmp {
      leaf enable {
        type boolean;
        description
          "Whether to enable SNMP. This will instantiate MIB objects for
           interfaces and pseudowires";
      }
      leaf interval {
        type uint8;
        default 5;
        description
          "The interval at which the SNMP objects will be synchronized with the
           state of the program.";
      }
    }

    grouping interface-address-families {
      container address-families {
        container ipv4 {
          leaf address {
            type inet:ipv4-address;
            description
              "The IPv4 address assigned to the L3 interface, without netmask.";
          }
          leaf next-hop {
            type inet:ipv4-address;
            description
              "The IPv4 next-hop address used for all packets leaving the L3
               interface. The address is assumed to be on-net.";
          }
        }
        container ipv6 {
          leaf address {
            type inet:ipv6-address;
             description
              "The IPv6 address assigned to the L3 interface, without netmask.";
          }
          leaf next-hop {
            type inet:ipv6-address;
            description
              "The IPv6 next-hop address used for all packets leaving the L3
               interface. The address is assumed to be on-net.";
          }
        }
      }
    }

    list interface {
      description
        "A list of interface definitions.";

      key "name";
      min-elements 1;

      leaf name {
        type string;
        mandatory true;
        description
          "The name of the interface. This name appears as the <ifName> and
           <ifDescr> SNMP OIDs";
      }

      leaf description {
        type string;
        description
          "A free-form description of the function of the interface. This string
           appears as the <ifAlias> SNMP OID";
      }

      container driver {
        leaf path {
          type string;
          mandatory true;
          description
            "The path of the Snabb driver to use for the interface in 'dotted'
             notation, e.g. 'apps.intel_mp.intel_mp'";
        }
        leaf name {
          type string;
          mandatory true;
          description
            "The name of the driver object to instantiate for the interface,
             relative to <path> (i.e. 'require(path)[name]'";
        }
        leaf config {
          type string;
          description
            "A string containing a literal Lua expression.  The result of the
             evaluation of this expression is passed as argument to
             the call of the driver's new() method";
        }
        leaf extra-config {
          description
            "A string containing a literal Lua expression which must evaluate to a
             table.  If the expression given in <config> is a table,
             it is merged with this table. If a key is present in both
             tables, the one from <extra-config> takes precedence.";
          type string;
        }
      }

      container mirror {
        description
          "Traffic mirror configuration for the interface.";

        leaf rx {
          type boolean;
          description
            "Whether to enable mirroring for incoming packets.";
        }
        leaf tx {
          type boolean;
          description
            "Whether to enable mirroring for outgoing packets.";
        }
        leaf type {
          type enumeration {
            enum "tap";
            enum "pcap";
          }
          default "tap";
          description
            "The method to use for providing access to mirrored traffic. If set to
             <tap>, the traffic is senf to a TAP interface where it
             can be collected with standard tools (e.g. tcpdump).  If
             no explicit <name> is given, the name of the TAP inteface
             is derived from the name of the mirrored interface as
             follows: slashes are replaced by hyphens and the
             direction ('rx' ot 'tx') is appended to the end of the
             name separeted by an underscore.  The name is truncated
             at the system's limit for interface names (the IFNAMSIZ
             constant, which currently has the value 16).

             If set to <pcap>, the mirrored packets are written to a
             file in pcap format.  If no explicit <name> is given, it
             defaults to '/tmp/' followed by the name derived from the
             interface as described above with an additional '.pcap'
             appended to the final path name";
        }
        leaf name {
          type string;
          description
            "If given, overrides the names of either the TAP interface or pcap file
             as described in <type>.  The name is augmented with the suffix '_rx' or
             '_tx', depending on the direction.";
        }
      }

      leaf mtu {
        type uint16;
        mandatory true;
        description
          "The MTU of the interface including the entire L2 header (e.g. 14 bytes
           for an untagged interface, 18 bytes for a single-tagged
           interface";
      }

      uses interface-address-families;

      container trunk {
        leaf enable {
          type boolean;
          mandatory true;
          description
            "Whether to configure the interface as a VLAN trunk";
        }
        leaf encapsulation {
          default "dot1q";
          type enumeration {
            enum dot1q;
            enum dot1ad;
            enum raw;
          }
          description
            "The type of encapsulation to use in VLAN tags. This determines the
             value of the 'Tag Protocol Identifier field'. For 'dot1q'
             and 'dot1ad' the values are 0x8100 and
             0x88a8. respectively.  It is possible to chose an
             arbitrary TPID by setting the encapsulation to 'raw' and
             setting <tpid> to the desired value.";
        }
        leaf tpid {
          type uint16;
          description
            "If <encapsulation> is 'raw', this is the number that will be used as
             the 'Tag Protocol Identifier'.";
        }

        list vlan {
          description
            "A list of VLANs to be transported on the trunk.";

          key "vid";
          min-elements 1;

          leaf vid {
            type uint16 {
              range 0..4095;
            }
            mandatory true;
            description
              "The VLAN ID.";
          }
          leaf description {
            type string;
            description
              "A free-form description of the function of the interface. This string
                 appears as the <description> SNMP OID";
          }
          leaf mtu {
            type uint16;
            description
              "The MTU of the sub-interface including the L2 header.  This number can
                 be smaller than the MTU of the underlying physical
                 interface but must not exceed it.";
          }

          uses interface-address-families;

        } // vlan
      } // trunk
    } // interface

    container peers {
      description
        "Definition of peers between which transport of L2 packets can be
         established.  There must be exactly one peer in the local
         list and at least one in the remote list.  Each peer has a
         list of IPv4 or IPv6 addresses associated with it.  Unique
         pairs of local and remote addresses can be defined as
         communication channels through the transport configuration item.";

      grouping peer {
        description
          "";

        leaf name {
          type string;
          description
            "The name of the local system.";
        }

        list endpoint {
          description
            "An ipv4 or ipv6 address which can be used as an endpoint for a
             pseudowire.";

          key "name";

          leaf name {
            type string;
            description
              "The name of the endpoint. The pair of peer name and endpoint name is
               used to select the remote and local addresses for a transport.";
          }

          choice address-by-family {
            mandatory true;
            case ipv4 {
              leaf ipv4 {
                type inet:ipv4-address;
                mandatory true;
                description
                  "An IPv4 address.";
              }
            }
            case ipv6 {
              leaf ipv6 {
                type inet:ipv6-address;
                mandatory true;
                description
                  "An IPv6 address.";
              }
            }
          }
        }
      }

      list local {
        min-elements 1;
        max-elements 1;

        key "name";
        uses peer;
      }

      list remote {
        min-elements 1;

        key "name";
        uses peer;
      }
    }

    list transport {
      min-elements 1;
      description
        "A transport is a pair of addresses selected from the set of endpoints
         of all configured peers, with the restriction that the local
         endpoint must be selected from the local peer and the remote
         endpoint must be selected from any of the remote peers.  The
         address families of the addresses must match the address
         family chosen for the transport.";

      key "name";

      grouping endpoint-selector {
        leaf peer {
          type string;
          mandatory true;
          description
            "The name of the peer from which to select an endpoint address.";
        }
        leaf endpoint {
          type string;
          mandatory true;
          description
            "The name of the endpoint relative to the selected peer.";
        }
      }

      leaf name {
        type string;
        mandatory true;
        description
          "The name of this transport.";
      }
      leaf address-family {
        type enumeration {
          enum "ipv4";
          enum "ipv6";
        }
        mandatory true;
      }
      container local {
        description
          "The selector of an address from the local peer.";
        uses endpoint-selector;
      }
      container remote {
        description
          "The selector of an address from any of the remote peers.";
        uses endpoint-selector;
      }
      container ipsec {
        leaf enable {
          type boolean;
          default false;
          description
            "Whether to enable IPsec for this transport.";
        }
        leaf encryption-algorithm {
          type enumeration {
            enum "aes-gcm-16-icv";
          }
          default aes-gcm-16-icv;
          description
            "The encryption algorithm to use.  Currently, only aes-gcm-16-icv is
             supported";
        }
      }
    }

    list vpls {
      description
        "A list of VPLS definitions.";

      key "name";

      leaf name {
        type string;
        mandatory true;
        description
          "The name of the VPLS instance.";
      }
      leaf description {
        type string;
        description
          "A brief description of the VPLS. TODO: MIB object";
      }
      leaf mtu {
        type uint16;
        mandatory true;
        description
          "The MTU of the L2 domain provided by the VPLS.  A VPLS instance will
           fail to start unless all of its attachment circuits have a
           matching MTU.  This value is also propagated throughout the
           control-channels of the pseudowires to remote instances of
           the VPLS. A pseudowire is considered to be in a 'down'
           state operationally if the MTUs at each endpoint do not
           match.";
      }
      leaf uplink {
        type string;
        mandatory true;
        description
          "The name of the interface used as the uplink for the VPLS as it
           appears in the 'interface' list.  This is effectively a
           static route for outbound packets.";
      }

      container bridge {
        description
          "The configuration for the bridge module associated with the VPLS to
             which all pseudowires and attachment circuits are
             connected.  The bridge module will not be created if the
             VPLS only contains a single pseusowire and a single
             attachment circuit.  In that case, the pseudowire is
             connected directly to the attachment circuit.

             As a mechanism for loop prevention, the bridge enforces a
             split-horizon policy on all ports to which a pseudowire
             connects: packets arriving from a pseudowire are never
             forwarded to any other pseudowire.  As a consequence, the
             pseudowires that make up a VPLS are required to form a
             full mesh.";

        choice type {
          case flooding {
            leaf flooding {
              type empty;
              description
                "A 'flooding' bridge forwards a copy of a frame received on a port to
                   all other port, only subject to the restriction of
                   the split-horizon policy.";
            }
          }
          case learning {
            container learning {
              description
                " A 'learning' bridge behaves like a flooding bridge for multicast
                    packets and packets addressed to unknown unicast
                    destinations.  The unicast source address of each
                    incoming frame is added to the list of known
                    addresses of the ingress port such that all future
                    packets destined to that address are only
                    transmitted on that particular port.";

              container mac-table {
                description
                  "Configuration of the MAC table associated with the learning bridge.";

                leaf size {
                  type uint32;
                  default 1024;
                  description
                    "The initial size (number of entries) of the MAC address table";
                }
                leaf timeout {
                  type uint16;
                  default 600;
                  description
                    "The interval in seconds after which an inactive entry is expired from
                     the MAC address table.";
                }
                leaf verbose {
                  type boolean;
                  default false;
                  description
                    "Whether to log periodic statistics about table usage and resize
                     events.";
                }
                leaf max-occupy {
                  type decimal64;
                  default 0.4;
                  description
                    "The maximum load-factor of the hash table before automatic re-sizing
                     is initiated.  The load factor is defined as the
                     ratio of the number of occupied entries and the
                     size of the table.";
                }
              } // mac-table
            }
          }
        } // choice
      } // bridge

      list attachment-circuit {
        key "name";

        leaf name {
          type string;
          mandatory true;
          description
            "An arbitrary name that identifies this attachment circuit.";
        }
        leaf interface {
          type string;
          mandatory true;
          description
            "The name of the interface as it appears in the 'interface' list.";
        }
      }

      list pseudowire {
        key "name";

        leaf name {
          type string;
          mandatory true;
          description
            "An arbitrary name that identifies this pseudowire.";
        }

        leaf vc-id {
          type uint16;
          mandatory true;
          description
            "The VC ID associated with this pseudowire. It is used to distinguish
             pseudowires between the same pair of addresses.  In other
             words, the triple (local address, remote address, VC ID)
             must be unique among the set of all pseudowires known to
             the system. The range of valid values depends on the
             tunnel protocol, because the VC ID has to be transported
             in the tunnel header.  Certain tunnels do not support a
             VC ID at all. In that case, the vc-id parameter must be
             set to zero and the uniqueness property applies to the
             pair (local address, remote address).";
        }

        leaf transport {
          type string;
          mandatory true;
          description
            "The name of the transport to use for this pseudowire.";
        }

        container tunnel {
          description
            "The type and configuration of the tunnel protocol. Only a single
             selection is allowed.  Currently supported tunnels are
             l2tpv3 (in the flavor of 'Keyed IPv6 Tunnel', RFC 8159)
             and GRE.";

          choice tunnel-type {
            mandatory true;
            case l2tpv3 {
              container l2tpv3 {
                description
                  "The configuration of the l2tpv3 tunnel type.";

                mandatory true;
                leaf local-cookie {
                  type string;
                  default "\x00\x00\x00\x00\x00\x00\x00\x00";
                  description
                    "The value of the local cookie (expected in the header of incoming
                     packets) as a binary string of exactly 8
                     characters.  The string is evaluated as a Lua
                     string and may contain any escape sequence
                     supported by Lua.";
                }
                leaf remote-cookie {
                  type string;
                  default "\x00\x00\x00\x00\x00\x00\x00\x00";
                  description
                    "The value of the remote cookie (written to the header of outgoing
                     packets) as a binary string of exactly 8
                     characters.  The string is evaluated as a Lua
                     string and may contain any escape sequence
                     supported by Lua.";
                }
              }
            }
            case gre {
              container gre {
                presence
                  "There are no configurable elements for this tunnel type.";
                description
                  "The GRE tunnel uses the value 0x6558 (Transparent Ethernet Bridging)
                   for the protocol header field.  The key field is
                   used to transport the VC ID. Checksumming is not
                   supported.";

                leaf dummy {
                  type boolean;
                  description
                    "This optional node is ignored. It exists to work around a bug
                     in the YANG parser which doesn't recognize an empty presence container";
                }
              }
            }
          }
        } // tunnel

        container control-channel {
          description
            "The configuration of the proprietary control-channel between tunnel
               endpoints.";

          leaf enable {
            type boolean;
            default true;
            description
              "Whether to enable the control-channel for this pseudowire.";
          }
          leaf heartbeat {
            type uint16;
            default 5;
            description
              "The interval in seconds at which heartbeat messages are sent to the
                 peer.";
          }
          leaf dead-factor {
            type uint16;
            default 3;
            description
              "The number of heartbeat intervals without receiving heartbeat messages
                 after which the peer is decalred to be dead.  Like
                 the tunnel itself, the operational status of a
                 pseudowire is unidirectional.";
          }
        } // control-channel
      } // pw
    } // vpls
  } // l2vpn-config
}
```
## <a name="interface-abstraction">Interface Abstraction</a>

Currently, the view of an interface in the Snabb architecture is
rather low-level since it only provides the drivers itself, which
basically presents a raw L2 interface to Snabb applications.  In order
to accommodate the abstractions of sub-interfaces via VLAN-trunks as
well as L3 interfaces used for VPLS uplinks, the L2VPN system provides
an additional layer of software on top of the drivers to provide a
view of the interfaces which resembles that of a traditional
networking device.

### Configuration Template

Each physical interface that is to be used by the `l2vpn` program is
configured by an instance of the `interface` list as shown in the YANG
schema above.

### Naming vs Addressing

Before discussing the configuration in detail, an important
distinction between the naming of an interface and its addressing as a
PCI device needs to be established.

In a traditional networking device, apart from supplying a unique
identification, interface names typically contain the type of
interface as well as its physical location in the chassis,
e.g. `TenGigE1/5`, which would refer to the 10GE port number 5 located
on the linecard in slot number 1.

In contrast to such devices, regular server hardware, on which the
Snabb application is expected to be used predominantly, usually
doesn't allow for a straight-forward scheme to describe the physical
location of an interface.  On the other hand, there already exists a
name that uniquely identifies every interface on any given system: its
PCI address.

It is tempting to use this address as the identity of an interface
throughout the system, but that would make it very hard to adopt a
different convention later on.  Therefore, it is necessary to make a
clear distinction between the name of an interface and its PCI address
and adpot the rule that any reference to an interface must use its
name and never its PCI address.

Then, we can adopt any convention we like for the naming of
interfaces, including using the PCI address verbatim, i.e. with a
configuration fragment like this:

```
interface {
  name "0000:03:00.0";
  driver {
    path "apps.intel_mp.intel_mp";
    name "Intel";
    config "{ pciaddr = '0000:03:00.0', rxq = 0, txq = 0 }";
     },
  },
}
```

Note that `config` must be a literal Lua expression.

This convention doesn't play well with the naming convention for
sub-interfaces introduced later on (due to the dot in the PCI address)
and doesn't appear as a particularly useful choice from an operator's
point of view in general.

It is possible to stick to the traditional convention and chose names
of the form

```
<type><n>[/<n>]...
```

where `<type>` could be one of the following

| Ethernet type  | `<type>`  |  
| -------------- | ----------|
| 1000BASE-*     | GigE      |
| 10GBASE-*      | TenGigE   |

The numbering scheme would then depend on the actual port layout of
the device and should be part of the device-specific documentation.
This is the convention we will use in the remainder of this document.

For example:

```
interface {
  name "TenGigE0/0";
  driver {
    path "apps.intel_mp.intel_mp";
    name "Intel";
    config "{ pciaddr = '0000:03:00.0', rxq = 0, txq = 0 }";
  }
}
```

To assign this interface as attachment circuit of some VPLS instance,
one would use a configuration like the following (see the complete
[configuration of a VPLS instance](#vpls-instance-config)).

```
vpls {
  name "vpls1";
  attachment-circuit {
    name "ac1";
    interface "TenGigE0/0";
  }
  .
  .
  .
}
```

### Basic Interface Configuration

The following items make up the basic configuration of an interface

```
name <name>;
description <description>;
driver {
  path <path>;
  name <name>;
  config "{ pciaddr = <pciaddress> }";
  extra_config <extra_config>;
},
```

The `name` and `pciaddr` items have already been discussed in the
previous section.  The `description` is a string describing the
purpose of the interface in the current configuration.  It defaults to
the empty string.

The `path` must be a string that represents the path to the Lua module
that implements the driver, while `name` must be a string that
identifies the Lua table within the module.  The driver will be loaded
by executing

```
require(<path>).<name>
```

For example, for the Intel NIC, `path` would be
`apps.intel_mp.intel_mp` and `name` would be `Intel`.  The result of
the `require` will be passed to the `confi.app()` API call to create
an instance of the driver.

The `config` will be evaluated as a Lua expression which is passed as
argument to the constructor of the driver module.  It is
driver-specific.  For a physical interface, it will typically include
at least the PCI address, for example

```
config "{ pciaddr = '0000:03:00.0' }"

```

Note: the device will be detached from the Linux kernel when the
driver is configured and will not be re-attached automatically when
the driver releases the device.  The device can be re-attached to the
Linux kernel by executing

```
echo "<pciaddress>" > /sys/bus/pci/drivers/<driver>/bind
```

as root, where `<driver>` is the name of the Linux driver that
controls the device (e.g. `igb` or `ixgbe`).

The driver of a physical interface usually requires the MTU to be set
on the link as a parameter.  However, the MTU is set as a generic
interface property via the `mtu` option as shown in the next section.
The MTU is automatically added to the `config` table with the key
`mtu`.

It is allowed to use a different data type than a table for the
`config` paramter to support non-standard drivers.  In that case, the
MTU cannot automatically be passed to the driver.

The optional element `extra_config` must also be a Lua expression
which must evaluate to a table.  It will be merged with the `config`
table to form the final configuration of the driver.  When elements
with the same name exist in both tables, the one from `extra_config`
takes precedence.  The main purpose of this feature is to support
Lua-agnostic front-ends that generate the entire `l2vpn` configuration
or parts of it from a higher layer of abstraction.

If `config` does not evaluate to a Lua table, `extra_config` is
ignored.

### L2 Configuration

Every physical interface is associated with parameters that control
its behavior at the Ethernet layer (L2) consisting of the items

```
mtu <mtu>;
trunk {
  enable true | false;
  encapsulation dot1q | dot1ad | raw;
  tpid <tpid>;
}
mirror {
  rx true | false;
  tx true | false;
  type tap | pcap;
}
```

The `vlan` section of the trunk configuration is not shown here and
will be discussed with the concept of
[sub-interfaces](#sub-interfaces).

<a name="MTU">The MTU (Maximum Transmission Unit)</a> is the maximum
number of bytes in an entire Ethernet packet that can be transmitted
on the link in either direction, i.e. the maximum receive unit (MRU)
is automatically chosen to be identical to the MTU.  It includes the
Ethernet header and payload but none of the other elements of a
complete Ethernet frame (preamble, SOF, CRC) nor the interpacket gap.

For untagged packets, the size of the Ethernet header is 14 bytes,
which must be included in the MTU.  For example, an MTU of 1514 bytes
is required to transport a payload of up to 1500 bytes.

For every level of VLAN tagging, an additional 4 bytes need to be
added to the MTU to accommodate the increased size of the Ethernet
header.  E.g., if plain 802.1Q is used and the interface should still
be able to transport 1500-byte IP packets, the MTU must be chosen as
1518 bytes.

Note that oversized packets are dropped silently but are counted in
the `ifInDiscards` and `ifOutDiscards` objects if SNMP is enabled.

The trunk configuration determines whether the link is carrying a
service-delimiting tag.  By definition, this is a tag which is
interpreted and removed by the `l2vpn` program itself as opposed to
tags that only have meaning to the customer and must be ignored by the
`l2vpn` program.

If `trunk/enable` is set to `false`, no service-delimiting tag is
expected on packets received from the interface (anything beyond the
source and destination MAC addresses is ignored) and no tag is added
when packets are transmitted to the interface.  This configuration
puts the physical interface into L2-mode where Ethernet frames are
simply forwarded without any manipulations performed on them.  In
conventional switches, this is sometimes referred to as a
_switchport_.  The following example shows a complete interface
configuration for this case:

```
interface {
  name "TenGigE0/0";
  driver {
  path "apps.intel_mp.intel_mp";
  name "Intel";
  config = "{ pciaddr = '0000:03:00.0', rxq = 0, txq = 0 }";
  mtu = 1514,
  trunk { enable false; }
},
```

If `trunk/enable` is set to `true`, a service-delimiting tag is
expected to be present immediately following the Ethernet header on
received packets (also referred to as the "outer" tag).  The
`encapsulation` field determines the value of the TPID field expected
in the tag as described [above](#ac-modes).  In this configuration,
so-called _sub-interfaces_ need to be defined as part of the trunk
configuration to establish which VLAN IDs are expected to be
transported on the trunk and how they should be processed by the
`l2vpn` application.  This is further discussed
[below](#sub-interfaces).

Packets whose Ethertype field do not match that of the configured
encapsulation, are silently dropped unless the ["native
VLAN"](#native-vlan) feature is enabled.

Note that even though only the outermost header is interpreted by the
`l2vpn` program when trunking is enabled for an interface, the MTU
must still cover the entire packet.  For example, if the customer is
carrying her own VLAN trunk on top of the service-delimiting tag, the
packets actually contain two tags and the MTU must be set to 1522
bytes in order for the customer to be able to transport 1500-byte IP
packets within her VLANs.

The `mirror` container is used to configure per-direction
port-mirroring of the traffic that is received from (`rx`) or sent to
(`tx`) the interface.  Mirroring is enabled by setting the
corresponding option to `true`.

If the `type` option is not specified or is set to `tap`, a `Tap`
device is created to which a copy of all packets in the selected
direction is sent.  Packets can be examined by the standatd tools
(`tcpdump`, `wireshark` etc.) applied to such an interface.  If the
mirror selector is set to `true`, the name of the `Tap` device is
derived from the name of the interface in the following manner.
First, all slashes are replaced by hyphens, then the name is truncated
to 13 characters (corresponding to the system limit `IFNAMSIZ`, which
is 16 on a Linux system, minus 3) and the string `_rx` or `_tx` is
appended, depending on the selected direction of traffic.  If the
`name` option is specified, it is used as the name of the interface
instead, with `_rx` or `_tx` appended and subjected to the same rules
of truncation to the maximum length.

If the `type` option is set to `pcap`, the packets are written to a
file in the `pcap` format.  If the `name` option is not set, the
filename is constructed by replacing slashes by hyphens in the name of
the interface and appending the string `_rx.pcap` or `_tx.pcap`,
depending on the selected direction of traffic.  The full path name is
constructed by prepending the file name with `/tmp/`.  If the `name`
option is set, it is used as the complete path name of the file
instead with `_rx.pcap` or `_tx.pcap` appended depending on the
direction of the mirrored traffic.

Examples: the fragment
```
interface {
  name "TenGigE1/1";
  mirror {
    rx true;
  }
}
```
will create the `Tap` device `TenGigE1-1_rx` and
```
interface {
  name "TenGigE1/1";
  mirror {
    rx true;
    tx true;
    type pcap;
    name "/tmp/foo";
  }
}
```
will create the files `/tmp/foo_rx.pcap` and `/tmp/foo_tx.pcap`.

Enabling port-mirroring incurs a substantial performance penalty due
to the packet replication and possibly system calls required to
transmit the mirrored packets to the TAP device.

### Address-family (L3) Configuration

A L2-port as discussed in the previous section can be turned into a
L3-port by adding a section containing address-family-specific
configurations:

```
interface {
  name <name>;
  [ description = <description>; ]
  driver {
    path <path>;
    name <name>;
    config "{
      pciaddr = <pciaddress>,
    }";
  }
  mtu <mtu>;
  trunk { enable false; }
  address-families {
    ipv6 {
      address <address>;
      next-hop <next_hop>;
      [ next-hop-mac <next_hop_mac>; ]
    }
    ipv4 {
      address <address>;
      next-hop <next_hop>;
      [ next-hop-mac <next_hop_mac>; ]
    }
  }
}
```

The `address-families` container can contain configurations of either
or both address families.  Both are restricted to a single address per
interface.

Like on a conventional system, the main configuration option is the
assignment of an address to the interface.  Currently, the notion of a
subnet mask is not supported, see below.

The presence of any L3 interface implies that the system also provides

   * Some kind of routing functionality which determines
     the address of a directly attached system to which a given
     packet must be handed off, a.k.a. _next-hop_

   * An address-resolution process that finds the MAC address of the
     interface for the next-hop, a.k.a. _neighbor discovery_ (provided
     by ICMPv6 and ARP for IPv6 and IPv4, respectively).  This process
     must also support address resolution in the other direction, when
     the adjacent system tries to discover the MAC address associated
     with the local address

Currently, the Snabb architecture has no concept of routing at all,
let alone any kind of dynamic routing.  Within the `l2vpn`
application, only the "uplinks" need to be L3-interfaces, because they
transport payload encapsulated in IP between the device and the ISP
network.  Attachment circuits are, by definition, L2-ports.

Because the `l2vpn` application essentially monopolizes all interfaces
that it uses, a generic routing mechanism is not necessary.

Packets arriving on an uplink are always destined for the `l2vpn`
application itself (any other packets except control-plane traffic
like neighbor discovery have no meaning to the system and need to be
dropped) and are fed directly into the network of applications that
processes them.

The IP packets that are generated by the `l2vpn` application itself by
encapsulating Ethernet frames from attachment circuits are directed to
a specific L3-port via the VPLS configuration to which the
corresponding pseudowire belongs (via the `uplink` item in the [VPLS
configuration](#vpls-instance-config)).  This can be viewed as a kind
of policy-based routing decision.  Each L3-port is associated with a
static next-hop which is essentially an interface-specific
default-route.  This is the purpose of the `<next_hop>` configuration
item, which must be a complete IP address. This is the only address
for which outbound neighbor discovery needs to be performed and it is
simply assumed that it is "on-net", i.e. directly reachable.  This is
why a subnet mask is not required.

The difference to a L2-port is the insertion of a _neighbor discovery_
module (ND) between the interface driver and the Snabb application
that attaches to the port.  The ND module performs the following
functions

   * For packets coming from the driver

     * Handle packets which are part of the ND protocol
       (i.e. address resolution)

     * Pass all other packets to the application or drop them if the
       type field in the Ethernet header doesn't have the proper value
       for the L3 protocol (e.g. `0x86dd` for IPv6 and `0x0800 for
       IPv4)

   * For packets coming from the application (they are expected
     to already contain an Ethernet header)

     * Set the type field in the Ethernet header to the proper
       value for the L3 protocol

     * Fill in the MAC address of the physical interface as
       source address in the Ethernet header

     * If the next-hop address has been resolved by ND,
       fill in the discovered MAC address as destination
       address in the Ethernet header

     * If ND for the next-hop has not completed, drop
       the packet

If the MAC address for `next-hop` is set via the `next-hop-mac`
configuration option, neighbor discovery need not be performed for
outgoing packets.

### <a name="sub-interfaces">Sub-Interfaces</a>

When trunking is enabled on a physical interface, one virtual
interface is created for every VLAN which is configured to be part of
the trunk.

Conceptually, a sub-interface represents a slice of the physical
interface where the (service-delimiting) VLAN tag is used as a
de-multiplexer: when a packet is received, its tag is removed and the
remaining packet is passed to the sub-interface for further
processing.  When a packet is transmitted by a sub-interface to the
physical interface, the corresponding tag is added to the packet
before transmission.

The previous two sections discussed how a physical interface can be
configured as either a L2- or L3-port in the absence of a trunk.  The
exact same configurations are available on the sub-interface level if
trunking is enabled.  In this case, the physical interface is not
allowed to have a `address-families` configuration section.

Sub-interfaces are created by adding one container per VLAN in the
trunk configuration:

```
trunk {
  enable true;
  encapsulation dot1q | dot1ad | raw;
  vlan {
    description = <description>, ]
    vid <vid>;
    [ mtu <mtu>; ]
    [ address-families {
         ipv6 {
            address <address>;
            next-hop <next_hop>;
            [ next-hop-mac <next_hop_mac>; ]
          },
          ipv4 {
            address <address>;
            next-hop <next_hop>;
            [ next-hop-mac =<next_hop_mac>; ]
          }
      }
    ]
  }
  vlan {
    ...
  }
  ...
}
```

The `<vid>` selects the VLAN ID to which the sub-interface is mapped.
It must be unique among the sub-interfaces of the same physical
interface and must be in the range 0-4094, where the ID 0 plays a
special role as explained below.  A sub-interface does not have a
`name` node.  Instead, its name is automatically constructed by
appending the `<vid>` to the name of the physical interface, separated
by a dot.  For example,

```
interface {
  name "TenGigE0/0";
  driver {
    path "apps.intel_mp.intel_mp";
    name "Intel";
    config "{ pciaddr = '0000:03:00.0', rxq = 0, txq = 0 }";
  }
  mtu 1518;
  trunk {
    enable true;
    encapsulation dot1q;
    vlan {
      vid 1;
    }
  }
}
```

will create the sub-interface named `TenGigE0/0.1` as a L2-port.  This
convention is chosen because it is the de-facto standard on
traditional devices and it is one of the reasons why using the PCI
address as an interface's name is not practical (due to the PCI
address containing a dot to separate the "function" element in the
standard syntax).  A sub-interface is configured as a L3-port exactly
like a physical port as explained in the previous section.

The optional `mtu` parameter can be used to assign an MTU to the
sub-interface which is smaller than the MTU of the physical interface.
If it is omitted, the sub-interface inherits the MTU from the physical
interface.

Note that the MTU is not enforced at L2, i.e. it only is effective for
services at higher layers.  For example, if the sub-interface carries
a L3 configuration, IP packets destined for that inerface must not
exceed the MTU of the sub-interface (subtracted by the L2 header).
Another example is the case when a sub-interface is used as an AC in a
VPLS, where the MTU of the VPLS must match the MTU of the
sub-interface.

#### <a name="native-vlan">Native VLAN</a>

It is possible to mix tagged and untagged traffic on a VLAN trunk
(always with respect to service-delimiting tags).  Untagged packets
can be mapped to a dedicated sub-interface that uses the reserved VLAN
ID 0.  For example,

```
interface {
  name "TenGigE0/0";
  driver {
    path "apps.intel_mp.intel_mp";
    name "Intel";
    config "{ pciaddr = '0000:03:00.0', rxq = 0, txq = 0 }";
  }
  mtu 1518;
  trunk {
    enable true;
    encapsulation dot1q;
    vlan {
      vid 0;
    }
    vlan {
      vid 1;
    }
  }
}
```

creates the sub-interfaces `TenGigE0/0.0` and `TenGigE0/0.1`.  Packets
that carry the 802.1Q VLAN ID 1 are assigned to `TenGigE0/0.1` while
packets, whose Ethertype is not equal to `0x8100` are assigned to
`TenGigE0/0.0` (packets that carry a 802.1Q VLAN ID other than 1 will
be dropped).

### <a name="SNMP">SNMP</a>

The L2VPN program does not provide an actual SNMP engine itself.
Instead, it stores the raw objects in a shared memory segment and
relies on an external program to make them available with SNMP.  A
[perl-based
program](https://github.com/alexandergall/snabb-snmp-subagent.git) is
available, which parses this data and can hook into any SNMP agent
that supports the AgentX protocol, like
[Net-SNMP](http://www.net-snmp.org/).

It is impoprtant to note that the SNMP data generated by the `l2vpn`
application only contains the objects themselves, not the full table
structure.  It is up to the SNMP agent to generate the proper index
structure from this data (e.g. `ifIndex` for the interface MIBs).

The L2VPN framework supports the following SNMP MIBs for monitoring:

   * `interfaces` (.1.3.6.1.2.1.2)
   * `ifMIB` (.1.3.6.1.2.1.31)
      * `ifXTable` (.1.3.6.1.2.1.31.1.1)
   * `cpwVcMIB` (.1.3.6.1.4.1.9.10.106)
      * `cpwVcObjects` (.1.3.6.1.4.1.9.10.106.1)
         * `cpwVcTable` (.1.3.6.1.4.1.9.10.106.1.2)
   * `pwStdMIB` (.1.3.6.1.2.1.10.246)
      * `pwObjects` (.1.3.6.1.2.1.10.246.1)
         * `pwTable` (.1.3.6.1.2.1.10.246.1.2)
   * `cpwVcEnetMIB` (.1.3.6.1.4.1.9.10.108)
     * `cpwVcEnetObjects` (.1.3.6.1.4.1.9.10.108.1)
        * `cpwVcEnetTable` (.1.3.6.1.4.1.9.10.108.1.1)
   * `pwEnetStdMIB` (1.3.6.1.2.1.180)
      * `pwEnetObjects` (1.3.6.1.2.1.180.1)
         * `pwEnetTable` (1.3.6.1.2.1.180.1.1)

#### Interface MIBs

If SNMP support is enabled, each physical interface creates a row in
the `interfaces` and `ifMIB` tables.  The assignment of the `ifDescr`,
`ifName` and `ifAlias` objects is of particular importance for network
management purposes.  These objects have a very long history and the
current usage is unfortunately not uniform among vendors.  The intent
of the specification is the following:

   * `ifDescr` A human readable description containing, e.g. the
     manufacturer, product name and the hardware/software version

   * `ifName` The textual name of the interface as used by the
      user interface of the device

   * `ifAlias` A description of the interface provided by the operator

By this definition, it should be the `ifName` object that would be
assigned the interface's name, e.g. `TenGigE0/0`.  Historically, only
`ifDescr` existed at first and was used for this purpose in violation
of the specification due to the lack of an object that was suited for
this purpose.  When `ifName` was added, most vendors continued to
(ab)use `ifDescr` and this is still the case at least for the industry
standard (i.e. Cisco Systems).

A common choice in practice is to set both, `ifDescr` and `ifName` to
the interface name.  This is the convention that the `l2vpn` program
uses as well.  usage of`ifDescr` from the interface name but it also
sets `ifName` to the same value objects from the interface name. The
`ifAlias` is populated from the `description` of the interface.  For
example

```
interface {
  name "TenGigE0/0";
  description "AC customer A";
  driver {
    path "apps.intel_mp.intel_mp";
    name "Intel";
    config "{ pciaddr = '0000:03:00.0', rxq = 0, txq = 0 }";
  }
  mtu 1514;
  trunk { enable false; }
}
interface {
  name "TenGigE0/1";
  description "AC customer B";
  driver {
    path "apps.intel_mp.intel_mp";
    name "Intel";
    config "{ pciaddr = '0000:03:00.1', rxq = 0, txq = 0 }";
  }
  mtu 1514;
  trunk { enable false; }
}
```

would create the objects

   * `ifDescr` = "TenGigE0/0"
   * `ifName`  = "TenGigE0/0"
   * `ifAlias` = "AC customer A"

and

   * `ifDescr` = "TenGigE0/1"
   * `ifName`  = "TenGigE0/1"
   * `ifAlias` = "AC customer B"

for those two interfaces, respectively.

#### PW MIBs

The `pw*` MIBs are part of the IETF reference architecture known as
_Pseudo Wire Emulation Edge-to-Edge_ (PWE3, [RFC
3985](https://www.ietf.org/rfc/rfc3985.txt "RFC 3985")).  As usual,
vendors have implemented various pre-standard (draft) versions of
these MIBs during the lengthy standardization process.  Some of them
still use those versions and didn't even bother to support the
standardized version at all, like Cisco.  The `cpw*` MIBs are used on
most Cisco devices and are almost but not quite identical to their
`pw*` counterparts.

## <a name="ipsec">IPsec</a>

A pseudowire can be protected by IPsec for authentication and
confidentiality.  An IPsec policy is applied to a specific transport.
This implies that all pseudowire using the same transport are covered
by the same policy.  Configuration-wise, IPsec is covered by the
`ipsec` container within a `transport`.

```
ipsec {
  enable true | false;
  encryption-algorithm aes-gcm-16-icv;
}
```

Currently, the only supported algorithm is `aes-gcm-16-icv`.
Authentication of the endpoints and the negotiation of keying material
must be supplied by an external service that implements the IKE
protocol.

### Dynamic Key Exchange

The `l2vpn` program relies on a modified version of the [Strongswan
IPsec suite](https://www.strongswan.org/) to provide authentication
and key exchange through their IKE daemon called `charon`.  The
[modified
version](https://github.com/alexandergall/strongswan/tree/kernel-snabb-5.6.3)
is based on release 5.6.3.

The IKE protocol requires connectivity between the PE devices over
regular (non-Snabb) interfaces to establish the security associations
(SA) used for IKE itself as well as the child-SAs used by `l2vpn` to
protect its pseudowires.

#### Building the `kernel-snabb` plugin

To build the `charon` plugin that supports Snabb applications, simply
add the option `--enable-kernel-snabb` to the invocation of
`configure` in the build recipe for you Linux distribution.

In the following, it is assumed that the native `systemd` variant of
`charon` is used (called `charon-systemd`).

#### Configuring IKE

The configuration of IKE associations is closely related to the list
of peers.  Essentially, one must establish one IKE SA per pair of
peers which is referenced by a transport that has IPsec enabled.


For the purpose of illustration, we assume the following setup of a
pair of systems with two pseudowires between them that should be
protected by IPsec.

   * System `A`

      * Regular IP address `10.0.1.1` used for IKE
      * Local address `192.168.1.1` for VPLS instance `vplsv4`
      * Local address `2001:db8:0:1::1` for VPLS instance `vplsv6`

   * System `B`

      * Regular IP address `10.0.2.1` used for IKE
      * Local address `192.168.2.1` for VPLS instance `vplsv4`
      * Local address `2001:db8:0:2::1` for VPLS instance `vplsv6`

The relevant part of the `l2vpn` configuration of endpoint `A` is as
follows

```
l2vpn-config {
  peers {
    local {
      name "A";
      endpoint {
        name "v4";
        ipv4 "192.168.1.1";
      }
      endpoint {
        name "v6";
        ipv6 "2001:db8:0:1::1";
      }
    }
    remote {
      name "B";
      endpoint {
        name "v4";
        ipv4 "192.168.2.1";
      }
      endpoint {
        name "v6";
        ipv6 "2001:db8:0:2::1";
      }

    }
  }
  transport {
    name "v4";
    address-family "ipv4";
    local {
      peer "A";
      endpoint "v4";
    }
    remote {
      peer "B";
      endpoint "v4";
    }
    ipsec {
      enable true;
    }
  }
  transport {
    name "v6";
    address-family "ipv6";
    local {
      peer "A";
      endpoint "v6";
    }
    remote {
      peer "B";
      endpoint "v6";
    }
    ipsec {
      enable true;
    }
  }
  vpls {
    name "vplsv4";
    vc-id 1;
    transport "v4";
    tunnel {
      gre {}
    }
  }
  vpls {
    name "vplsv6";
    vc-id 0;
    transport "v6";
    tunnel {
      l2tpv3 {}
    }
  }
}
```

On system `B`

```
l2vpn-config {
  peers {
    local {
      name "B";
      endpoint {
        name "v4";
        ipv4 "192.168.2.1";
      }
      endpoint {
        name "v6";
        ipv6 "2001:db8:0:2::1";
      }
    }
    remote {
      name "A";
      endpoint {
        name "v4";
        ipv4 "192.168.1.1";
      }
      endpoint {
        name "v6";
        ipv6 "2001:db8:0:1::1";
      }

    }
  }
  transport {
    name "v4";
    address-family "ipv4";
    local {
      peer "B";
      endpoint "v4";
    }
    remote {
      peer "A";
      endpoint "v4";
    }
    ipsec {
      enable true;
    }
  }
  transport {
    name "v6";
    address-family "ipv6";
    local {
      peer "B";
      endpoint "v6";
    }
    remote {
      peer "A";
      endpoint "v6";
    }
    ipsec {
      enable true;
    }
  }
  vpls {
    name "vplsv4";
    vc-id 1;
    transport "v4";
    tunnel {
      gre {}
    }
  }
  vpls {
    name "vplsv6";
    vc-id 0;
    transport "v6";
    tunnel {
      l2tpv3 {}
    }
  }
}
```

The `charon` daemon requires two configuration files,
`strongswan.conf` and `swanctl.conf`.  The former must contain

```
charon-systemd {
  plugins {
    kernel-snabb {
      load = 1000
    }
  }
}
```

in order to prefer the Snabb plugin over the regular kernel plugin
(usually `kernel-netlink`).

The actual IKE configuration is provided by `swanctl.conf`. It must
define one `connection` per IKE peer and one child-SA per pseudowire.
In this example, the configuration on system `A` could look as
follows.

```
connections {
  system_B {
    local_addrs = 10.0.1.1
    remote_addrs = 10.0.2.1
    version = 2
    remote-B {
      auth = psk
      id = vpls_psk
    }
    local-A {
      auth = psk
      id = vpls_psk
    }
    proposals = aes128-sha256-x25519-esn

    children {
      vplsv4 {
        esp_proposals = aes128gcm128-x25519-esn
        local_ts = 192.168.1.1/32
        remote_ts = 192.168.2.1/32
        mode = tunnel
      }
      vplsv6 {
        esp_proposals = aes128gcm128-x25519-esn
        local_ts = 2001:db8:0:1::1/128
        remote_ts = 2001:db8:0:2::1/128
        mode = tunnel
      }
    }
  }
}
secrets {
  ike-vpn {
    id-main = vpls_psk
    secret = 0sFpZAZqEN6Ti9sqt4ZP5EWcqx
  }
}

```

This assumes that a pre-shared key (PSK) is used to authenticate the
IKE peers (any other authentication mechanism supported by Strongswan
could be used as well).

The configuration must meet the following requirements.

   * The proposal must include an algorithm of type `aes128gcm128`
     with a suitable Diffie-Hellman group (in this example the
     elliptic curve 25519)

   * The proposal must include "extended sequence numbers" (`esn`)

   * Each child must contain exactly one traffic selector for
     `local_ts` and `remote_ts`

   * The traffic selector must be a host address (/32 or /128)

   * The traffic selectors must match exactly the addresses configured
     in the `traffic-selector` of the `l2vpn` configuration.

   * The `mode` must be set to `tunnel`

Here, the configuration of system `B` would look like

```
connections {
  system_A {
    local_addrs = 10.0.2.1
    remote_addrs = 10.0.1.1
    version = 2
    remote-A {
      auth = psk
      id = vpls_psk
    }
    local-B {
      auth = psk
      id = vpls_psk
    }
    proposals = aes128-sha256-x25519-esn

    children {
      vplsv4 {
        esp_proposals = aes128gcm128-x25519-esn
        local_ts = 192.168.2.1/32
        remote_ts = 192.168.1.1/32
        mode = tunnel
      }
      vplsv6 {
        esp_proposals = aes128gcm128-x25519-esn
        local_ts = 2001:db8:0:2::1/128
        remote_ts = 2001:db8:0:1::1/128
        mode = tunnel
      }
    }
  }
}
secrets {
  ike-vpn {
    id-main = vpls_psk
    secret = 0sFpZAZqEN6Ti9sqt4ZP5EWcqx
  }
}

```

#### Initiation, re-keying

For a successful key-exchange, the `l2vpn` program must already be
running.  The exchange can then be initiated on the command line using

```
$ swanctl --initiate --child vplsv4
```

and

```
$ swanctl --initiate --child vplsv6
```

A successful initiation should look like this:

```
[IKE] establishing CHILD_SA vplsv6{2}
[ENC] generating CREATE_CHILD_SA request 0 [ N(ESP_TFC_PAD_N) SA No KE TSi TSr ]
[NET] sending packet: from 10.0.1.1[500] to 10.0.2.1[500] (304 bytes)
[NET] received packet: from 10.0.2.1[500] to 10.0.1.1[500] (304 bytes)
[ENC] parsed CREATE_CHILD_SA response 0 [ N(ESP_TFC_PAD_N) SA No KE TSi TSr ]
[IKE] received ESP_TFC_PADDING_NOT_SUPPORTED, not using ESPv3 TFC padding
[IKE] CHILD_SA vplsv6{2} established with SPIs 00000101_i 00000123_o and TS 2001:DB8:0:1::1/128 === 2001:DB8:0:2::1/128
initiate completed successfully
```

The connection can be initialized by either side. Re-keying of the IKE
SA happens automatically. Re-keying of the child SAa is not necessary
but can be triggered manually if desired, e.g. by

```
$ swanctl --rekey --child vplsv6
```

Initiataion must be performed whenever the `l2vpn` is started or
restarted.

## External Routing

As has already been discussed, only static routing is supported on the
uplink interface by configuring the next-hop address used for all
outgoing packets.  This address must be resolvable to a MAC address
through regular IP neighbor discovery by the adjacent router.  There
currently is no concept of a routing table in the Snabb architecture,
but this mechanism is essentially equivalent to a static default
route.

For the tunnels to work, all local IP endpoint addresses need to be
reachable by the remote sides of the pseudowires.  How this is
achieved is outside the scope of the Snabb system.  There are
basically two methods.

   * The adjacent router configures static routes for each IP address
     that terminates a VPLS and re-distributes the routes within the
     routing domains in which the remote endpoints are located.

   * The host running the VPNTP announces the local endpoint addresses
     with the next-hop address of the local uplink interface address
     to the provider's routing system with BGP, from where it needs to
     be re-distributed in such a manner that the addresses are
     reachable by the remote endpoints.

The second method requires IP connectivity between the host and a
suitable BGP-speaking router in the network over a regular (non-Snabb
controlled) interface.  Two typical scenarios for this method are the
following.

   * The VPNTP host is configured with a private AS number and
     maintains a BGP session to one or more routers in its vicinity
     (e.g. the adjacent router itself).  These routers must
     re-distribute the advertised prefixes as appropriate.

   * If the provider uses an internal BGP (iBGP) setup with
     route-reflectors, the VPNTP host can be configured directly as
     route-reflector client within the provider's autonomous system.
